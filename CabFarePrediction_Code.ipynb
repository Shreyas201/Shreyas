{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from random import randrange, uniform\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the working directory depending on where the csv file is stored\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"train_cab.csv\", encoding=\"ISO-8859-1\")\n",
    "training_data = training_data.replace([\"\", \" \", \"NAN\"], np.NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with missing values\n",
    "missing_val = pd.DataFrame(training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index\n",
    "missing_val = missing_val.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename variables\n",
    "missing_val = missing_val.rename(columns = {'index' : 'variables', 0: 'Missing_percentage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage\n",
    "missing_val['Missing_percentage'] = (missing_val['Missing_percentage']/len(training_data))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descending order\n",
    "missing_val = missing_val.sort_values('Missing_percentage', ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output results\n",
    "missing_val.to_csv(\"Missing_perc.csv\", index = False)\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laying a framework to choose best method for imputing missing value\n",
    "# Record the data\n",
    "# Actual Value = 40.71\n",
    "# median       = 40.75\n",
    "# mean         = 39.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a missing value to choose the best method for imputation of missing values\n",
    "training_data['pickup_latitude'].loc[2] = np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation with mean method\n",
    "training_data['pickup_latitude'] = training_data['pickup_latitude'].fillna(training_data['pickup_latitude'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation with median method\n",
    "training_data['pickup_latitude'] = training_data['pickup_latitude'].fillna(training_data['pickup_latitude'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the created missing value with actual value\n",
    "training_data['pickup_latitude'].loc[2] = 40.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only two variables 'passenger_count' and 'fare_amount' has missing value\n",
    "# Median method is very close to the actual value, hence median method is chosen to impute missing value\n",
    "training_data[\"passenger_count\"] = training_data[\"passenger_count\"].fillna(training_data[\"passenger_count\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \"fare_amount\" is of object type it cannot be imputed using mean or median\n",
    "training_data['fare_amount'] = training_data['fare_amount'].fillna(training_data['fare_amount'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck for the missing values\n",
    "missing_val_new = pd.DataFrame(training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Analysis using boxplot method\n",
    "# first store the data in another variable \n",
    "df = training_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot visualise outliers\n",
    "%matplotlib inline\n",
    "# boxplot can be plotted for individual variables as shown below\n",
    "plt.boxplot(training_data[\"dropoff_latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numeric names\n",
    "cnames = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and delete outliers from data\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    q75, q25 = np.percentile(training_data.loc[:,i], [75,25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    min = q25 - (1.5*iqr)\n",
    "    max = q75 + (1.5*iqr)\n",
    "    print(min)\n",
    "    print(max)\n",
    "    \n",
    "    training_data = training_data.drop(training_data[training_data.loc[:,i] < min].index)\n",
    "    training_data = training_data.drop(training_data[training_data.loc[:,i] > max].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After outliers are removed the number of observations dropped from 16067 to 9457 using boxplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing model to check for collinearity among the variables and to train the model\n",
    "# Performing correlation Analysis\n",
    "num_corr = training_data.loc[:,cnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width and height of the plot\n",
    "h, wd = plt.subplots(figsize=(7,5))\n",
    "\n",
    "# Generate correlation Matrix\n",
    "corr = num_corr.corr()\n",
    "\n",
    "# Plot using seaborn library\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            square=True, ax=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passenger_count is not correltaed to any one of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test of independence\n",
    "chi2, p , dof, ex = chi2_contingency(pd.crosstab(training_data[\"fare_amount\"],training_data[\"pickup_datetime\"]))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since 'p' value is greater than 0.05 we approve the null hypothesis i.e it is not correlated to the dependent variable\n",
    "# and exclude \"pick_datetime\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction i.e deleting variables with are not correlated to target variable and have no impact on output\n",
    "training_data_deleted = training_data.drop([\"pickup_datetime\", \"passenger_count\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normality Check\n",
    "%matplotlib inline\n",
    "plt.hist(training_data[\"pickup_longitude\"],bins = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the histogram we decide to scale the data using standardisation\n",
    "# Save numeric names\n",
    "cnames = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]\n",
    "cnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data using standardisation\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    training_data_deleted[i] = (training_data_deleted[i] - training_data_deleted[i].mean())/training_data_deleted[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Decision Tree for training and testing data\n",
    "# dividing into train and test data by 80-20\n",
    "train, test = train_test_split(training_data_deleted, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree regression\n",
    "fit_DT = DecisionTreeRegressor(max_depth = 2).fit(train.iloc[:,2:5],train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model on test data\n",
    "predictions_DT = fit_DT.predict(test.iloc[:,2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Error\n",
    "def MAPE(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred)/y_true))\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(test.iloc[:,1], predictions_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error rate = 4.09%\n",
    "# Accuracy = 95.91%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
